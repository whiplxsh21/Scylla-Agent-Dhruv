{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatmodels with Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda, RunnableSequence\n",
    "from langchain.schema.runnable import RunnableParallel, RunnableBranch\n",
    "\n",
    "\n",
    "import textwrap #for wrapping cell outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='models/gemini-1.5-pro' google_api_key=SecretStr('**********') client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001AD4C95F5D0> default_metadata=()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "llm = ChatGoogleGenerativeAI(model='gemini-1.5-pro')\n",
    "#legitimate key: \"AIzaSyBG1l0UG8RgeF93JblR4R9g7ihDYzGbkNU\"\n",
    "\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can access and process information from the real world through Google Search and keep my response consistent with search results. Therefore, I am updated continuously.  However, I don't have a memory of past updates or a specific \"last updated\" date.  I'm more like a search engine with conversational abilities.\n",
      "\n",
      "To demonstrate my access to current information, here's a quick summary of some recent news:\n",
      "\n",
      "* **Global Politics:**  Tensions remain high in Eastern Europe, with ongoing diplomatic efforts to address the situation.  (This is a deliberately broad summary, as the specifics evolve daily.  If you want details, ask me a more specific question.)\n",
      "* **Economy:** Inflation and interest rates continue to be key concerns for many countries, with central banks taking various actions.  Supply chain issues are also still impacting global trade.\n",
      "* **Technology:**  Developments in artificial intelligence continue at a rapid pace, with new models and applications emerging frequently.  Discussions around AI regulation and ethical considerations are also prominent.\n",
      "\n",
      "For more specific news, please ask me a question about the topic you're interested in.  For example, you could ask:\n",
      "\n",
      "* \"What's the latest news on inflation in the US?\"\n",
      "* \"What are the recent developments in AI research?\"\n",
      "* \"What's happening with the situation in Ukraine?\"\n",
      "\n",
      "I can then provide you with a more detailed and relevant response based on the latest information available.\n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(\"Hi, when was the last time you were updated? can you provide recent news?\")\n",
    "print(result.content) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing chat history for context\n",
    "\n",
    "SystemMessage: defines the AI's role at the start of the conversation. eg: \"You are a finance expert.\"\\\n",
    "HumanMessage: user input or queries/questions asked to the AI. eg: \"Give me a list of investment banks.\"\\\n",
    "AIMessage: contains AI response based on previous messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't \"detect\" fraud in the way a real-time fraud detection system does. I'm a large language\n",
      "model, not connected to any live financial systems.  I can't access personal data or transaction\n",
      "details.  Think of me as an informational resource, not a fraud investigator.  However, I can tell\n",
      "you about the tools and methods *real* fraud detection systems use, and the signs that might\n",
      "indicate fraudulent activity.  **Methods and Tools used in real-world fraud detection:**  * **Rule-\n",
      "based systems:** These systems use predefined rules to flag suspicious transactions.  For example, a\n",
      "rule might flag any transaction over $10,000 from a new IP address.  These are effective for\n",
      "catching common fraud patterns but can be less effective against new and evolving tactics. *\n",
      "**Anomaly detection:** This uses statistical models to identify transactions that deviate\n",
      "significantly from normal behavior.  For example, if someone typically makes small purchases online\n",
      "and suddenly makes a large purchase from a different country, it could be flagged.  Machine learning\n",
      "plays a big role here. * **Predictive modeling:** These models use historical data to predict the\n",
      "likelihood of future fraud.  They consider various factors like transaction amount, location, time,\n",
      "and past behavior. * **Link analysis:**  This explores relationships between different entities\n",
      "(individuals, accounts, devices) to uncover hidden networks of fraud.  For example, if multiple\n",
      "seemingly unrelated accounts all withdraw money from the same ATM, it could indicate a coordinated\n",
      "fraud attempt. * **Device fingerprinting:** This tracks unique characteristics of devices used to\n",
      "access online accounts, helping to identify potentially compromised accounts. * **Behavioral\n",
      "biometrics:** This analyzes user behavior, such as typing speed, mouse movements, and scrolling\n",
      "patterns, to identify deviations from normal behavior that could indicate fraud. * **Real-time\n",
      "transaction monitoring:** Systems constantly monitor transactions as they occur, allowing for\n",
      "immediate intervention if fraud is suspected.   **Metrics used in fraud detection:**  * **False\n",
      "Positive Rate:** The percentage of legitimate transactions incorrectly flagged as fraudulent. *\n",
      "**False Negative Rate:** The percentage of fraudulent transactions that go undetected. *\n",
      "**Precision:**  Out of all the transactions flagged as fraudulent, how many actually were\n",
      "fraudulent. * **Recall:** Out of all the actually fraudulent transactions, how many were correctly\n",
      "flagged. * **Accuracy:** The overall correctness of the fraud detection system. * **AUC (Area Under\n",
      "the Curve) of the ROC (Receiver Operating Characteristic):**  A measure of the model's ability to\n",
      "distinguish between fraudulent and legitimate transactions.  **What you can do to protect\n",
      "yourself:**  * **Monitor your accounts regularly.**  Check for any unauthorized transactions. *\n",
      "**Use strong passwords and enable multi-factor authentication.** * **Be cautious of phishing\n",
      "scams.** Don't click on links or open attachments from unknown senders. * **Keep your software\n",
      "updated.**  Security updates often patch vulnerabilities that can be exploited by fraudsters. *\n",
      "**Report suspicious activity immediately.**  Remember, I can't analyze your specific situation for\n",
      "fraud. If you suspect fraudulent activity, contact your financial institution immediately. They have\n",
      "the tools and resources to investigate and take appropriate action.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(\"You are a financial fraud detecting AI agent, you immediately signal when can fraud be detected and also educate people on the same. you know of a lot of methods that people use.\"),\n",
    "    HumanMessage(\"How do you detect fraud? what tools and metrics do you use?\")\n",
    "]\n",
    "\n",
    "result = llm.invoke(messages)\n",
    "wrapped_text = textwrap.fill(result.content, width=100) #text is the object which you want to print\n",
    "print(wrapped_text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Localised version of chatgpt using gemini in terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "llm2 = ChatGoogleGenerativeAI(model='gemini-1.5-pro')\n",
    "system_message = SystemMessage(content=\"You are an expert in poker, you wont answer any question unrelated to it. Politely say no and steer the conversation otherwise.\")\n",
    "chat_history.append(system_message)\n",
    "\n",
    "while True:\n",
    "    query = input(\"You: \")\n",
    "    if query.lower() == 'exit':\n",
    "        break\n",
    "    chat_history.append(HumanMessage(content=query))\n",
    "    \n",
    "    result = llm2.invoke(chat_history)\n",
    "    response = result.content\n",
    "    chat_history.append(AIMessage(content=response))\n",
    "    \n",
    "    print(\"AI Response: \\n\")\n",
    "    wrapped_text = textwrap.fill(response, width=100) #text is the object which you want to print\n",
    "    print(wrapped_text)\n",
    "    \n",
    "print(\"------------- convo ended ----------\")\n",
    "print()\n",
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "\n",
    "example: with just one human message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Subject: Fired up for ML/Backend at ModusAI!  Hey ModusAI, LangChain & FastAPI are my jam – I'm\n",
      "stoked about your ML/Backend opening and ready to build something awesome.  My resume's attached;\n",
      "let's chat!\n"
     ]
    }
   ],
   "source": [
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "mod = ChatGoogleGenerativeAI(model='gemini-1.5-pro') #model or llm we are using\n",
    "\n",
    "\n",
    "template = \"Write a {tone} email to {company} expressing interest in the {position} position, mentioning {skill} as a key strength. Keep it to 4 lines maximum.\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template=template)\n",
    "prompt = prompt_template.invoke({\n",
    "    \"tone\" : \"energetic\",\n",
    "    \"company\" : \"ModusAI\",\n",
    "    \"position\": \"ML and backend\",\n",
    "    \"skill\" : \"langchain, fastapi\"\n",
    "})\n",
    "\n",
    "result = mod.invoke(prompt)\n",
    "\n",
    "print()\n",
    "wrapped_text = textwrap.fill(result.content, width=100) #text is the object which you want to print\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "another example: with one human message and one system message\\\n",
    "(we can create this kind of prompt using tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What kind of car does an egg drive?  A Yolkswagen.\n",
      "\n",
      "2. Why did the mechanic sleep under the car? He wanted to wake up oily in the morning.\n",
      "\n",
      "3.  How do you make a car disappear?  You just gotta \"car-pe\" diem!  *Poof!*\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    (\"system\", \"You are a comedian who tells really bad jokes about {topic}.\"),\n",
    "    (\"human\", \"Tell me {joke_count} jokes.\")\n",
    "]\n",
    "\n",
    "prompt_template_2 = ChatPromptTemplate.from_messages(messages=messages)\n",
    "prompt = prompt_template_2.invoke({\n",
    "    \"topic\": \"cars\",\n",
    "    \"joke_count\":3\n",
    "})\n",
    "\n",
    "result = mod.invoke(prompt)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains with Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example: you need to design an ML pipeline in which you have to create a prompt template, make a prompt, pass the final prompt to the LLM, get the LLM's response, and then convert that response into French and give it to the user.\\\n",
    "there will be steps involved to do this, we can use chains to sow them together into one workflow.\n",
    "- create a prompt template -> placeholder addition with invoke()\n",
    "- pass the final prompt to the LLM -> get the result\n",
    "- convert the result to french -> return to user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sequential chaining:** when one task needs to be completed after the other, you chain them one by one. the results of ith tasks are usually used as inputs for i+1th task.\\\n",
    "**parallel chaining:** when the tasks are unrelated, so you can do some number of tasks all at once and then return the result directly\\\n",
    "**conditional chaining:** if the user selects a certain thing or a certain thing happens, an entire different chain might be executed, example a troubleshooting chain if a user wants it, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. **The \"Dead Man's Hand\" is Aces and Eights:**  While not statistically any worse than other starting hands, the hand of two black aces and two black eights is famously known as the \"Dead Man's Hand\" because it was reportedly the hand Wild Bill Hickok was holding when he was shot and killed in 1876. This association has cemented its place in poker lore, even though its actual impact on the game is purely psychological.\n",
      "\n",
      "2. **Royal Flushes are rarer than Straight Flushes (but not by much):**  A Royal Flush (Ten through Ace of the same suit) is technically a type of Straight Flush.  However, because a Royal Flush is the highest possible Straight Flush, it's often considered a separate hand ranking.  The odds of hitting a Royal Flush are slightly lower than hitting any other Straight Flush because there's only one possible combination of cards that makes a Royal Flush, whereas there are four possible combinations for each other Straight Flush (e.g., 9-K, 8-Q, 7-J etc. of a given suit).\n",
      "\n",
      "3. **Poker involves a blend of skill and luck:** Although luck plays a significant role in the short term (the cards you are dealt), skill determines long-term success.  Skilled players understand probabilities, bet sizing, bluffing strategies, reading opponents (tells), and managing their bankroll. These skills allow them to minimize losses when luck is against them and maximize winnings when it's in their favor.  Over a large enough sample of hands, skill will prevail over random chance.\n"
     ]
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a {topic} expert and know facts about this {topic} in great detail.\"),\n",
    "        (\"human\", \"Tell me {fact_count} facts.\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "model = ChatGoogleGenerativeAI(model='gemini-1.5-pro') #model or llm we are using\n",
    "\n",
    "\n",
    "#chain definition, takes prompt_template, makes it with variables, passes final prompt into model, then extracts content\n",
    "chain = prompt_template | model | StrOutputParser()\n",
    "\n",
    "result = chain.invoke({\"topic\":\"poker\", \"fact_count\":3})\n",
    "print(result) #no need to do result.content here because it is covered in the last step of chain already"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### runnable lambda and runnable sequence\n",
    "this is the same thing as using chains above, but instead of creating a chain with pipe operations, we are first making mini functions (runnables) with RunnableLambda and then chaining them with RunnableSequence\\\n",
    "we can use runnable lambda as a wrapper for small and repetitive tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eg: formatting of prompts can be done with this\n",
    "format_prompt = RunnableLambda(lambda x: prompt_template.format_prompt(**x))\n",
    "# (format_prompt method replaces the placeholder values in the prompt, so we can pass it directly into model.invoke)\n",
    "invoke_model = RunnableLambda(lambda y: model.invoke(y.to_messages()))\n",
    "parse_output = RunnableLambda(lambda z: z.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after we have the runnable mini-functions, we can chain them together with a runnablesequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. **The \"Dead Man's Hand\" is Aces and Eights:**  Wild Bill Hickok, a famous gunslinger and gambler, was holding two black aces and two black eights (along with an unknown fifth card, likely a five of diamonds or a jack of diamonds) when he was shot dead during a poker game in Deadwood, Dakota Territory.  While not a particularly strong hand in poker, it became legendary as the \"Dead Man's Hand\" and is often referenced in popular culture.\n",
      "\n",
      "2. **Poker is a game of skill, not just luck:** While luck plays a role in the short term (what cards you're dealt), skill determines long-term success.  Skill in poker involves understanding probabilities, reading opponents, bluffing effectively, managing your bankroll, and adapting your strategy to different game situations. Professional poker players consistently outperform amateurs over time, demonstrating the significant impact of skill.\n",
      "\n",
      "3. **There are hundreds of poker variations:**  While Texas Hold'em dominates the popular imagination, there's a vast world of poker games out there.  These include Omaha, Seven-Card Stud, Five-Card Draw, Razz, Badugi, and many more.  Each variation has its own unique set of rules, strategies, and optimal approaches, offering diverse and engaging gameplay for poker enthusiasts.\n"
     ]
    }
   ],
   "source": [
    "chain = RunnableSequence(first = format_prompt, middle=[invoke_model], last=parse_output)\n",
    "response = chain.invoke({\"topic\":\"poker\", \"fact_count\":3})\n",
    "print(response) #no need to do response.content here because it is covered in the last step of chain already"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(prefer pipe operator as it is less confusing, dont use lambdas or runnables unless you need to (you wont need to :))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Chaining\n",
    "make a chain to give two facts about something and convert it into french."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. **\"डेड मैन्स हैंड\" यानी मुर्दे का हाथ, इक्के और अट्ठे होते हैं।**  विशेष रूप से, काले इक्के और काले अट्ठे, जो वाइल्ड बिल हिकॉक के हाथ में थे जब उन्हें एक पोकर खेल के दौरान गोली मार दी गई थी।  यह एक दुखद और प्रसिद्ध हाथ है, लेकिन सांख्यिकीय रूप से यह संभावना के मामले में किसी भी अन्य दो-कार्ड वाले शुरुआती हाथ से अलग नहीं है।\n",
      "\n",
      "2. **फोर ऑफ़ अ काइंड, फुल हाउस से बड़ा होता है।**  हालांकि एक फुल हाउस (तीन एक जैसे और एक जोड़ा) शक्तिशाली लगता है, यह वास्तव में फोर ऑफ़ अ काइंड से नीचे स्थान पर है। ऐसा इसलिए है क्योंकि फोर ऑफ़ अ काइंड सांख्यिकीय रूप से बहुत दुर्लभ है।\n"
     ]
    }
   ],
   "source": [
    "poker_facts_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"you are an expert in {topic}. you like telling small facts about {topic}\"),\n",
    "    (\"human\", \"give me {count_facts} facts.\")\n",
    "])\n",
    "\n",
    "translation_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"you are an expert translator, you translate everything from english to {language}\"),\n",
    "    (\"human\", \"translate the following text to {language}: \\n {text}\")\n",
    "])\n",
    "\n",
    "prepare_for_translation = RunnableLambda(lambda output: {\"text\":output, \"language\": \"hindi\"})\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "model = ChatGoogleGenerativeAI(model='gemini-1.5-pro') #model or llm we are using\n",
    "\n",
    "fullchain = (poker_facts_template | model | StrOutputParser() | \n",
    "             prepare_for_translation | translation_template | model |StrOutputParser()\n",
    "             )\n",
    "\n",
    "#run the complete sequential chain\n",
    "sequential_result = fullchain.invoke({\"topic\": \"poker\", \"count_facts\":2})\n",
    "print(sequential_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Chaining\n",
    "when i kick off the process, theres n chains going to be kicked off parallely.\\\n",
    "ie, parallel chaining is n sequential chains, chain_1, chain_2... chain_n going parallely and then i will combine their outputs in someway and end my process.\\\n",
    "example: write a blog post about the movie 'inception', it should have three main components: a plot analysis, a character analysis, a component about the cast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "model = ChatGoogleGenerativeAI(model='gemini-1.5-pro')\n",
    "\n",
    "summary_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert movie summariser, you provide summaries of movies with key components like plot analysis, character analysis, cast, etc. You do it in about 700-1000 words.\"),\n",
    "    (\"human\", \"provide a summary of the movie: {movie_name}\")\n",
    "])\n",
    "\n",
    "def analyse_plot(plot):\n",
    "    plot_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are an expert movie critic.\"),\n",
    "        (\"human\", \"Analyse the plot: {plot} \\n What are its strengths and weaknesses? explain in 200 words max.\")\n",
    "    ])\n",
    "    return plot_template.format_prompt(plot=plot).to_messages()\n",
    "plot_branch_chain = (\n",
    "    RunnableLambda(lambda x: analyse_plot(x)) | model | StrOutputParser()\n",
    ")\n",
    "\n",
    "def analyse_characters(characters):\n",
    "    character_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are an expert movie critic\"),\n",
    "        (\"human\", \"Analyse the characters: {characters} \\n explain in 200 words max.\")\n",
    "    ])\n",
    "    return character_template.format_prompt(characters=characters).to_messages()\n",
    "character_branch_chain = (\n",
    "    RunnableLambda(lambda x: analyse_characters(x)) | model |StrOutputParser()\n",
    ")\n",
    "\n",
    "def analyse_cast(cast):\n",
    "    cast_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are an expert movie critic.\"),\n",
    "        (\"human\", \"List and talk about major actors/actresses in the cast: {cast}. Max 100 words.\")\n",
    "    ])\n",
    "    return cast_template.format_prompt(cast=cast).to_messages()\n",
    "cast_branch_chain = (\n",
    "    RunnableLambda(lambda x: analyse_cast(x)) | model | StrOutputParser()\n",
    ")\n",
    "\n",
    "def combine_verdicts(plot_analysis, character_analysis, cast_analysis):\n",
    "    return f\"Plot analysis: \\n{plot_analysis}\\n\\nCharacter Analysis:\\n{character_analysis}\\n\\nCast Analysis:\\n{cast_analysis}\\n\"\n",
    "\n",
    "parallel_chain = (\n",
    "    summary_template |\n",
    "    model |\n",
    "    StrOutputParser() |\n",
    "    RunnableParallel(branches={\"plot\":plot_branch_chain,\"characters\":character_branch_chain,\"cast\":cast_branch_chain}) |\n",
    "    #returns an object x, with x[\"branches\"] as a list x[\"branches\"][\"plot\"] to access the plot etc.\n",
    "    RunnableLambda(lambda x: combine_verdicts(x[\"branches\"][\"plot\"], x[\"branches\"][\"characters\"], x[\"branches\"][\"cast\"]))\n",
    ")\n",
    "\n",
    "parallel_result = parallel_chain.invoke({\"movie_name\": \"Inception\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot analysis: \n",
      "*Inception*'s greatest strength lies in its ambitious, layered narrative and dream-within-a-dream structure.  Nolan masterfully crafts a complex, suspenseful plot that keeps the audience guessing while exploring profound themes of guilt, memory, and reality. The strong ensemble cast delivers compelling performances, and the visual effects are breathtaking, creating truly immersive dream worlds.  The film's ambiguous ending is a bold choice, sparking endless debate and adding to its rewatch value.\n",
      "\n",
      "However, the film's complexity can also be a weakness. The sheer density of information and rapid-fire exposition can be overwhelming, potentially leaving some viewers confused or detached.  While emotionally resonant, character development sometimes takes a backseat to the intricate plot mechanics.  Additionally, some may find the reliance on exposition, particularly through Ariadne's character, a bit heavy-handed.  Despite these minor flaws, *Inception* remains a remarkable achievement in filmmaking, pushing the boundaries of the genre and leaving a lasting impact on popular culture.\n",
      "\n",
      "Character Analysis:\n",
      "*Inception*'s characters are intricately woven into its dream-within-a-dream narrative. Dom Cobb, haunted by his past, leads the team, driven by a desire to reunite with his children. Ariadne, the architect, serves as the audience surrogate, navigating and questioning the dream world's mechanics. Arthur, the pragmatist, provides logistical support and a grounded perspective.  Eames, the forger, adds an element of unpredictable flair and deception. Saito, the client, embodies ambiguous motivations and the influence of power. Robert Fischer Jr., the target, represents vulnerability and the potential for subconscious change. Mal, Cobb's deceased wife, manifests as a destructive projection of his guilt, blurring dream and reality.\n",
      "\n",
      "Each character plays a crucial role in the layered heist structure, their individual skills and motivations intertwined with the mission's success.  Cobb's emotional journey towards redemption, Ariadne's growing awareness, and the team's dynamic interactions contribute to the film's exploration of reality, subconscious power, and the human condition.  The casting, featuring Leonardo DiCaprio, Joseph Gordon-Levitt, Ellen Page, and Tom Hardy, further enhances the characters' complexity and emotional depth.\n",
      "\n",
      "Cast Analysis:\n",
      "Leonardo DiCaprio delivers a nuanced performance as Cobb, the haunted protagonist grappling with guilt and a blurred reality.  Joseph Gordon-Levitt provides a grounded counterpoint as the meticulous Arthur. Ellen Page shines as Ariadne, the audience surrogate navigating the dream world's complexities. Tom Hardy brings a charismatic unpredictability to Eames, the dream forger. Ken Watanabe adds gravitas as Saito, the enigmatic client. Cillian Murphy portrays Fischer, the vulnerable target, with compelling sensitivity. Marion Cotillard is mesmerizing as Mal, Cobb's tragic projection, embodying the destructive power of the subconscious.  This ensemble cast elevates *Inception* with their compelling portrayals of complex characters.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(parallel_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Chaining\n",
    "\n",
    "design the logic for a customer feedback agent, if the feedback is positive/negative/neutral then respond according to that, but if it is very bad or scathing then determine weather to forward/escalate it to a human agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "model = ChatGoogleGenerativeAI(model='gemini-1.5-flash')\n",
    "\n",
    "classification_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI agent.You respond well to positive feedback regarding the product you are selling.\"),\n",
    "    (\"human\", \"classify the sentiment of the feedback youve received: {feedback}.\\n Classify it as either positive, negative, neutral or escalate. (escalate to be used when the feedback is so negative that you need to escalate it to a human agent.)\")\n",
    "])\n",
    "\n",
    "positive_feedback_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI agent. You respond well to positive feedback regarding the product you are selling\"),\n",
    "    (\"human\", \"Generate a thank you note for the positive feedback youve received: {feedback}\")\n",
    "])\n",
    "negative_feedback_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI agent. You respond well to negative feedback regarding the product you are selling. DONT FORWARD TO A HUMAN AGENT IF IT IS NOT TOO BAD, just generate a response on your own in that case.\"),\n",
    "    (\"human\", \"Generate a response addressing this negative feedback: {feedback}\")\n",
    "])\n",
    "neutral_feedback_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI agent. You respond well to feedback regarding the product you are selling.\"),\n",
    "    (\"human\", \"Generate a response acknowledging this neutral feedback: {feedback}\")\n",
    "])\n",
    "escalate_feedback_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI agent. You respond well to feedback regarding the product you are selling. But if it is way too negative and cannot be handled by you, you will escalate it to a human agent. ONLY ESCALATE IF THE REVIEW IS VERY BAD/SCATHING. otherwise this will waste time for the human agent.\"),\n",
    "    (\"human\", \"generate a message to escalate this feedback to a human agent. {feedback}\")\n",
    "])\n",
    "\n",
    "\n",
    "branches = RunnableBranch( \n",
    "    (\n",
    "        lambda x: \"positive\" in x, \n",
    "        positive_feedback_template | model |StrOutputParser() \n",
    "    ),\n",
    "    (\n",
    "        lambda x: \"negative\" in x,\n",
    "        negative_feedback_template | model | StrOutputParser()\n",
    "    ),\n",
    "    (\n",
    "        lambda x: \"neutral\" in x,\n",
    "        neutral_feedback_template | model | StrOutputParser()\n",
    "    ),\n",
    "    escalate_feedback_template | model | StrOutputParser()\n",
    ") #this chain is like a switch case kind of condition, based on one word output from classification chain\n",
    "\n",
    "\n",
    "classification_chain = classification_template | model |StrOutputParser()\n",
    "chain = classification_chain | branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: Urgent: Escalation Request - Negative Customer Feedback Regarding Order # [Order Number]\n",
      "\n",
      "Body:\n",
      "\n",
      "Dear Human Agent,\n",
      "\n",
      "I've received negative feedback regarding order # [Order Number] that requires your immediate attention. The customer's feedback expresses significant dissatisfaction with both the delivery process and the product's functionality.  The severity of the complaint suggests it cannot be adequately addressed through automated responses. Please review the attached feedback and take appropriate action.\n",
      "\n",
      "Attached: [Customer Feedback - include the full text of the customer's complaint]\n",
      "-----------------\n",
      "This feedback was generated for: Negative.  This feedback clearly expresses dissatisfaction with both the delivery and the product's functionality.  It needs to be addressed.\n"
     ]
    }
   ],
   "source": [
    "reviews = {\"pos\": \"I love this product! thank you so much!\",\n",
    "           \"neg\": \"i didnt really like the delivery.. slightly malfunctional now\",\n",
    "           \"neutral\" : \"it was ok, just gets the job done i guess, nothing special\",\n",
    "           \"escalate\": \"The product is terrible, it broke just after one use and the quality is poor. I want a return issued immediately.\"\n",
    "           }\n",
    "result = chain.invoke({\"feedback\": reviews[\"neg\"]})\n",
    "print(result)\n",
    "print(\"-----------------\")\n",
    "typeoffeedback = classification_chain.invoke({\"feedback\": reviews[\"neg\"]})\n",
    "print(f\"This feedback was generated for: {typeoffeedback}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
